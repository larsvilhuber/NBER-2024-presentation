# Background

## Anecdotal evidence with SDS

From conversations/informal surveys:

- researchers were happy with the ability to access data without having to request a full-blown project in an FSRDC
- somewhat frustrated by the process (slowness)

## Reproducibility and SDS

SDS validation required typically substantial human debugging

- Reason:  problems with the reproducibility of code in the social sciences despite similarity of environment. 

## Intermediate causes

- no strong pre-testing of reproducibility, often intense use of interactive programming practices
- divergence in environments over time
- divergence of data schemas over time

**Failure to maintain strong links**

## Some broader evidence

In a sample of over **8,000 replication packages** associated with high-profile economics articles, **only 30% had some sort of master script**. 

## Other systems

Statistical agencies and research institutes have explored various ways to scale up access to confidential data, without full (remote) access to confidential data.

- Statistics Canada: RTRA process, 
- Norway:  [Microdata.no](microdata.no) system, 
- Germany/IAB: JoSuA system


## Limitations of other systems

Most such processes have limitations, including in their utility for general purpose analysis

- Most still have some **strong access limitations**
  - RTRA: 
  - microdata.no: Institutional MOU (and ony Norwegian residents)
  - IAB: proposal process

- Many systems strongly **limit the type of analysis** that is feasible by 
  - restricting the software keywords that can be used (RTRA), 
  - by creating a structured new statistical language (microdata.no)

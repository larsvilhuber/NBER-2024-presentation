% Encoding: ISO-8859-1


@article{reiterVerificationServersEnabling2009,
  title = {Verification Servers: {{Enabling}} Analysts to Assess the Quality of Inferences from Public Use Data},
  author = {Reiter, Jerome P and Oganian, Anna and Karr, Alan F},
  year = {2009},
  month = feb,
  journal = {Computational statistics \& data analysis},
  volume = {53},
  number = {4},
  pages = {1475--1482},
  issn = {0167-9473},
  doi = {10.1016/j.csda.2008.10.006},
  abstract = {To protect confidentiality, statistical agencies typically alter data before releasing them to the public. Ideally, although generally not done, the agency also provides a way for secondary data analysts to assess the quality of inferences obtained with the released data. Quality measures can help secondary data analysts to identify inaccurate conclusions resulting from the disclosure limitation procedures, as well as have confidence in accurate conclusions. We propose a framework for an interactive, web-based system that analysts can query for measures of inferential quality. As we illustrate, agencies seeking to build such systems must consider the additional disclosure risks from releasing quality measures. We suggest some avenues of research on limiting these risks.}
}



@article{AbowdSchmutte_BPEA2015,
	jstor_articletype = {research-article},
	title = {Economic analysis and statistical disclosure limitation},
	Author = {John M. Abowd and Ian Schmutte},
	journal = {Brookings Papers on Economic Activity},
	volume = {Fall 2015},
	url = {http://www.brookings.edu/about/projects/bpea/papers/2015/economic-analysis-statistical-disclosure-limitation},
	ISSN = {00072303},
	abstract = {This paper explores the consequences for economic research of methods used by statistical agencies to protect confidentiality of their respondents. We first review the concepts of statistical disclosure limitation for an audience of economists who may be unfamiliar with these methods. Our main objective is to shed light on the effects of statistical disclosure limitation for empirical economic research. In general, the standard approach of ignoring statistical disclosure limitation leads to incorrect inference. We formalize statistical disclosure methods in a model of the data publication process. In the model, the statistical agency collects data from a population, but published a version of the data that have been intentionally distorted. The model allows us to characterize what it means for statistical disclosure limitation to be ignorable, and to characterize what happens when it is not. We then consider the effects of statistical disclosure limitation for regression analysis, instrumental variable analysis, and regression discontinuity design. Because statistical agencies do not always report the methods they use to protect confidentiality, we use our model to characterize settings in which statistical disclosure limitation methods are discoverable; that is, they can be learned from the released data. We conclude with advice for researchers, journal editors, and statistical agencies.},
	language = {English},
	year = {2015},
	publisher = {Brookings Institution Press},
	copyright = {Copyright Â© 2015 Brookings Institution Press},
}

@Book{Dwork:Algorithmic:Book:2014,
	Title                    = {The Algorithmic Foundations of Differential Privacy},
	Author                   = {Cynthia Dwork and Aaron Roth},
	Publisher                = {now publishers, Inc.},
	Year                     = {2014},
	Note                     = {Also published as "Foundations and Trends in Theoretical Computer Science" Vol. 9, Nos. 3--4 (2014) 211-407.},
	
	Pages                    = {211--407},
	Url                      = {doi:10.1561/0400000042}
}
@Manual{synthpop,
	title = {synthpop: Generating Synthetic Versions of Sensitive Microdata for
	Statistical Disclosure Control},
	author = {Beata Nowok and Gillian M Raab and Joshua Snoke and Chris Dibben},
	year = {2016},
	note = {R package version 1.2-1},
	url = {https://CRAN.R-project.org/package=synthpop},
}

@InProceedings{kennickell1998multiple,
  author    = {Kennickell, Arthur B},
  title     = {Multiple imputation in the Survey of Consumer Finances},
  booktitle = {Proceedings of the Section on Survey Research Methods},
  year      = {1998},
  owner     = {vilhuber},
  timestamp = {2017.07.04},
}

@Article{kennickell1997multiple,
  author    = {Kennickell, Arthur B},
  title     = {Multiple imputation and disclosure protection: The case of the 1995 Survey of Consumer Finances},
  journal   = {Record Linkage Techniques},
  year      = {1997},
  volume    = {1997},
  pages     = {248--267},
  owner     = {vilhuber},
  timestamp = {2017.07.04},
}

@InProceedings{kennickell1991imputation,
  author    = {Kennickell, Arthur B},
  title     = {Imputation of the 1989 Survey of Consumer Finances: Stochastic relaxation and multiple imputation},
  booktitle = {Proceedings of the Survey Research Methods Section of the American Statistical Association},
  year      = {1991},
  volume    = {1},
  number    = {10},
  owner     = {vilhuber},
  timestamp = {2017.07.04},
}

@InProceedings{hawala2008producing,
  author    = {Hawala, Sam},
  title     = {Producing partially synthetic data to avoid disclosure},
  booktitle = {Proceedings of the Joint Statistical Meetings. Alexandria, VA: American Statistical Association},
  year      = {2008},
  owner     = {vilhuber},
  timestamp = {2017.07.04},
}

@Article{Ashwin2008,
  author    = {Ashwin Machanavajjhala and Daniel Kifer and John M. Abowd and Johannes Gehrke and Lars Vilhuber},
  title     = {Privacy: {T}heory meets practice on the map},
  journal   = {International Conference on Data Engineering (ICDE)},
  year      = {2008},
  pages     = {277--286},
  acmid     = {1547184},
  address   = {Washington, DC, USA},
  doi       = {10.1109/ICDE.2008.4497436},
  numpages  = {10},
  owner     = {vilhuber},
  publisher = {IEEE Computer Society},
  timestamp = {2017.07.04},
  url       = {http://dx.doi.org/10.1109/ICDE.2008.4497436},
}

@TechReport{Abowd2009NSFCensusIRS,
  author      = {Abowd, John M. and Andersson, Fredrik and Graham, Matthew and Vilhuber, Lars and Wu, Jeremy},
  title       = {Formal Privacy Guarantees and Analytical Validity of OnTheMap Public-use Data},
  institution = {NSF-Census-IRS Workshop on Synthetic Data and Confidentiality Protection 2009},
  year        = {2009},
  type        = {Presentation},
  owner       = {vilhuber},
  timestamp   = {2017.07.04},
  url         = {http://www.vrdc.cornell.edu/news/nsf-census-irs-workshop2009/program/},
}

@Misc{OnTheMap402,
  author       = {{U.S. Census Bureau}},
  title        = {OnTheMap version 3.1},
  howpublished = {online},
  year         = {2010},
  note         = {as of July 1, 2017},
  owner        = {vilhuber},
  timestamp    = {2017.07.04},
  url          = {https://lehd.ces.census.gov/announcements.html#091108},
}

@Article{ReiterJPC2010,
  author    = {Reiter, Jerome P.},
  title     = {Multiple Imputation for Disclosure Limitation: Future Research Challenges},
  journal   = {Journal of Privacy and Confidentiality},
  year      = {2010},
  volume    = {1},
  number    = {2},
  abstract  = {Statistical agencies that disseminate data to the public are ethically and often legally
required to protect the confidentiality of respondents' identities and sensitive attributes.
To satisfy these requirements, Rubin (1993), Little (1993), and Fienberg (1994) proposed
that agencies utilize multiple imputation. For example, agencies can release the units
originally surveyed with some values, such as sensitive values at high risk of disclosure
or values of key identifiers, replaced with multiple imputations. Multiple imputation
for protecting confidentiality is often called the synthetic data approach.},
  owner     = {vilhuber},
  timestamp = {2017.07.04},
  url       = {http://repository.cmu.edu/jpc/vol1/iss2/7},
}

@TechReport{Stinson2009NSFCensusIRS,
  author      = {Stinson, Martha and Benedetto, Gary and Bjelland, Melissa},
  title       = {Summary of Methods and Preliminary Assessment of the SIPP Synthetic Beta},
  institution = {NSF-Census-IRS Workshop on Synthetic Data and Confidentiality Protection 2009},
  year        = {2009},
  type        = {Presentation},
  owner       = {vilhuber},
  timestamp   = {2017.07.04},
  url         = {http://www.vrdc.cornell.edu/news/nsf-census-irs-workshop2009/program/},
}

@Article{SnokeEtAl2017,
  author        = {Joshua Snoke and Gillian Raab and Beata Nowok and Chris Dibben and Aleksandra Slavkovic},
  title         = {General and specific utility measures for synthetic data},
  year          = {2017},
  __markedentry = {[vilhuber:]},
  abstract      = {Data holders can produce synthetic versions of datasets when concerns about potential disclosure restrict the availability of the original records. This paper is concerned with methods to judge whether such synthetic data have a distribution that is comparable to that of the original data, what we will term general utility. We consider how general utility compares with specific utility, the similarity of results of analyses from the synthetic data and the original data. We adapt a previous general measure of data utility, the propensity score mean-squared-error (pMSE), to the specific case of synthetic data and derive its distribution for the case when the correct synthesis model is used to create the synthetic data. Our asymptotic results are confirmed by a simulation study. We also consider two specific utility measures, confidence interval overlap and standardized difference in summary statistics, which we compare with the general utility results. We present two examples examining this comparison of general and specific utility to real data syntheses and make recommendations for their use for evaluating synthetic data.},
  date          = {2016-04-22},
  eprint        = {1604.06651v2},
  eprintclass   = {stat.AP},
  eprinttype    = {arXiv},
  file          = {online:http\://arxiv.org/pdf/1604.06651v2:PDF},
  keywords      = {stat.AP},
  owner         = {vilhuber},
  timestamp     = {2017.07.04},
}

@Article{Meng1994,
  author        = {Xiao-Li Meng},
  title         = {Multiple-Imputation Inferences with Uncongenial Sources of Input},
  journal       = {Statistical Science},
  year          = {1994},
  volume        = {9},
  number        = {4},
  pages         = {538-558},
  issn          = {08834237},
  __markedentry = {[vilhuber:6]},
  abstract      = {Conducting sample surveys, imputing incomplete observations, and analyzing the resulting data are three indispensable phases of modern practice with public-use data files and with many other statistical applications. Each phase inherits different input, including the information preceding it and the intellectual assessments available, and aims to provide output that is one step closer to arriving at statistical inferences with scientific relevance. However, the role of the imputation phase has often been viewed as merely providing computational convenience for users of data. Although facilitating computation is very important, such a viewpoint ignores the imputer's assessments and information inaccessible to the users. This view underlies the recent controversy over the validity of multiple-imputation inference when a procedure for analyzing multiply imputed data sets cannot be derived from (is "uncongenial" to) the model adopted for multiple imputation. Given sensible imputations and complete-data analysis procedures, inferences from standard multiple-imputation combining rules are typically superior to, and thus different from, users' incomplete-data analyses. The latter may suffer from serious nonresponse biases because such analyses often must rely on convenient but unrealistic assumptions about the nonresponse mechanism. When it is desirable to conduct inferences under models for nonresponse other than the original imputation model, a possible alternative to recreating imputations is to incorporate appropriate importance weights into the standard combining rules. These points are reviewed and explored by simple examples and general theory, from both Bayesian and frequentist perspectives, particularly from the randomization perspective. Some convenient terms are suggested for facilitating communication among researchers from different perspectives when evaluating multiple-imputation inferences with uncongenial sources of input.},
  owner         = {vilhuber},
  publisher     = {Institute of Mathematical Statistics},
  timestamp     = {2017.07.04},
  url           = {http://www.jstor.org/stable/2246252},
}

@Article{DrechslerReiter2009,
  author    = {Drechsler, J\"org and Reiter, Jerome P.},
  title     = {Disclosure risk and data utility for partially synthetic data: An empirical study using the {German} {IAB} establishment survey.},
  journal   = {Journal of Official Statistics},
  year      = {2009},
  volume    = {25},
  pages     = {589--603},
  owner     = {vilhuber},
  timestamp = {2017.07.04},
}

@Article{synthpop2016,
  author    = {Nowok, Beata and Raab, Gillian M. and Dibben, Chris},
  title     = {synthpop : Bespoke creation of synthetic data in R},
  journal   = {Journal of Statistical Software},
  year      = {2016},
  volume    = {74},
  pages     = {1--26},
  owner     = {vilhuber},
  timestamp = {2017.07.04},
  url       = {https:
//www.jstatsoft.org/article/view/v074i11},
}

@techreport{burman2018,
author = {Leonard E. Burman and Alex Engler and Surachai Khitatrakun and James R. Nunns and Sarah Armstrong and John Iselin and Graham MacDonald and Philip Stallworth},
title = {Administrative Tax Data: Creating a Synthetic Public Use File and a Validation Server},
institution = {Tax Policy Center, Urban Institute and Brookings Institution},
url = {https://www.urban.org/research/publication/safely-expanding-research-access-administrative-tax-data-creating-synthetic-public-use-file-and-validation-server},
year = {2018},
type = {document},
}

@ARTICLE{2016arXiv160406651S,
	author = {{Snoke}, Joshua and {Raab}, Gillian and {Nowok}, Beata and
	{Dibben}, Chris and {Slavkovic}, Aleksandra},
	title = "{General and specific utility measures for synthetic data}",
	journal = {arXiv},
	keywords = {Statistics - Applications},
	year = "2017",
	month = "June",
	eid = {arXiv:1604.06651},
	pages = {arXiv:1604.06651},
	archivePrefix = {arXiv},
	eprint = {1604.06651},
	primaryClass = {stat.AP},
	adsurl = {https://ui.adsabs.harvard.edu/abs/2016arXiv160406651S},
	adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018arXiv180509392S,
	author = {{Snoke}, Joshua and {Slavkovi{\'c}}, Aleksandra},
	title = "{pMSE Mechanism: Differentially Private Synthetic Data with Maximal Distributional Similarity}",
	journal = {arXiv},
	keywords = {Statistics - Methodology},
	year = "2018",
	month = "May",
	eid = {arXiv:1805.09392},
	pages = {arXiv:1805.09392},
	archivePrefix = {arXiv},
	eprint = {1805.09392},
	primaryClass = {stat.ME},
	adsurl = {https://ui.adsabs.harvard.edu/abs/2018arXiv180509392S},
	adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@InProceedings{10.1007/978-3-319-99771-1_10,
	author="Snoke, Joshua	and Slavkovi{\'{c}}, Aleksandra",
	editor="Domingo-Ferrer, Josep	and Montes, Francisco",
	title="pMSE Mechanism: Differentially Private Synthetic Data with Maximal Distributional Similarity",
	booktitle="Privacy in Statistical Databases",
	year="2018",
	publisher="Springer International Publishing",
	address="Cham",
	pages="138--159",
	abstract="We propose a method for the release of differentially private synthetic datasets. In many contexts, data contain sensitive values which cannot be released in their original form in order to protect individuals' privacy. Synthetic data is a protection method that releases alternative values in place of the original ones, and differential privacy (DP) is a formal guarantee for quantifying the privacy loss. We propose a method that maximizes the distributional similarity of the synthetic data relative to the original data using a measure known as the pMSE, while guaranteeing {\$}{\$}{\backslash}epsilon {\$}{\$}-DP. We relax common DP assumptions concerning the distribution and boundedness of the original data. We prove theoretical results for the privacy guarantee and provide simulations for the empirical failure rate of the theoretical results under typical computational limitations. We give simulations for the accuracy of linear regression coefficients generated from the synthetic data compared with the accuracy of non-DP synthetic data and other DP methods. Additionally, our theoretical results extend a prior result for the sensitivity of the Gini Index to include continuous predictors.",
	isbn="978-3-319-99771-1"
}

@article{xiao-li-congenial,
author = {Xiao-Li Meng},
title = {{Multiple-Imputation Inferences with Uncongenial Sources of Input}},
volume = {9},
journal = {Statistical Science},
number = {4},
publisher = {Institute of Mathematical Statistics},
pages = {538 -- 558},
abstract = {Conducting sample surveys, imputing incomplete observations, and analyzing the resulting data are three indispensable phases of modern practice with public-use data files and with many other statistical applications. Each phase inherits different input, including the information preceding it and the intellectual assessments available, and aims to provide output that is one step closer to arriving at statistical inferences with scientific relevance. However, the role of the imputation phase has often been viewed as merely providing computational convenience for users of data. Although facilitating computation is very important, such a viewpoint ignores the imputer's assessments and information inaccessible to the users. This view underlies the recent controversy over the validity of multiple-imputation inference when a procedure for analyzing multiply imputed data sets cannot be derived from (is "uncongenial" to) the model adopted for multiple imputation. Given sensible imputations and complete-data analysis procedures, inferences from standard multiple-imputation combining rules are typically superior to, and thus different from, users' incomplete-data analyses. The latter may suffer from serious nonresponse biases because such analyses often must rely on convenient but unrealistic assumptions about the nonresponse mechanism. When it is desirable to conduct inferences under models for nonresponse other than the original imputation model, a possible alternative to recreating imputations is to incorporate appropriate importance weights into the standard combining rules. These points are reviewed and explored by simple examples and general theory, from both Bayesian and frequentist perspectives, particularly from the randomization perspective. Some convenient terms are suggested for facilitating communication among researchers from different perspectives when evaluating multiple-imputation inferences with uncongenial sources of input.},
keywords = {Congeniality, importance sampling, incomplete data, missing data, nonresponse, Normalizing constants, public-use data file, Randomization, self-efficiency},
year = {1994},
doi = {10.1214/ss/1177010269},
URL = {https://doi.org/10.1214/ss/1177010269}
}



@article{perla2021,
  title = {Equilibrium {{Technology Diffusion}}, {{Trade}}, and {{Growth}}},
  author = {Perla, Jesse and Tonetti, Christopher and Waugh, Michael E.},
  year = {2021},
  month = jan,
  journal = {American Economic Review},
  volume = {111},
  number = {1},
  pages = {73--128},
  issn = {0002-8282},
  doi = {10.1257/aer.20151645},
  url = {https://pubs.aeaweb.org/doi/10.1257/aer.20151645},
  urldate = {2022-11-14},
  abstract = {We study how opening to trade affects economic growth in a model where heterogeneous firms can adopt new technologies already in use by other firms in their home country. We characterize the growth rate using a summary statistic of the profit distribution: the mean-min ratio. Opening to trade increases the profit spread through increased export opportunities and foreign competition, induces more rapid technology adoption, and generates faster growth. Quantitatively, these forces produce large welfare gains from trade by increasing an inefficiently low rate of technology adoption and economic growth. (JEL D21, D24, F14, F43, O33)},
  langid = {english},
  timestamp = {2022-11-14T02:40:52Z},
  file = {Perla et al_2021_Equilibrium Technology Diffusion, Trade, and Growth.pdf:/home/vilhuber/Zotero/storage/LA7YQ3JQ/Perla et al_2021_Equilibrium Technology Diffusion, Trade, and Growth.pdf:application/pdf}
}



@Comment{jabref-meta: databaseType:bibtex;}
